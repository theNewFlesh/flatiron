<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>flatiron.tf.models.unet &mdash; flatiron  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/style.css" />

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../../_static/documentation_options.js?v=5929fcd5"></script>
        <script src="../../../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            flatiron
          </a>
              <div class="version">
                0.20.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../readme.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../readme.html#installation-for-developers">Installation for Developers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../readme.html#installation-for-production">Installation for Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../readme.html#quickstart-guide">Quickstart Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../readme.html#development-cli">Development CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../readme.html#production-cli">Production CLI</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../core.html">core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tf.html">tf</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development CLI</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cli.html">cli</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">flatiron</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">flatiron.tf.models.unet</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for flatiron.tf.models.unet</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="kn">import</span> <span class="n">keras</span>  <span class="c1"># noqa F401</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">keras</span><span class="w"> </span><span class="kn">import</span> <span class="n">KerasTensor</span>  <span class="c1"># noqa F401</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">lunchbox.enforce</span><span class="w"> </span><span class="kn">import</span> <span class="n">Enforce</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">keras</span><span class="w"> </span><span class="kn">import</span> <span class="n">layers</span> <span class="k">as</span> <span class="n">tfl</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">keras</span><span class="w"> </span><span class="kn">import</span> <span class="n">models</span> <span class="k">as</span> <span class="n">tfmodels</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pyd</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">flatiron.core.pipeline</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ficp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">flatiron.core.tools</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">fict</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">flatiron.core.validators</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">vd</span>
<span class="c1"># ------------------------------------------------------------------------------</span>


<span class="n">PAD</span> <span class="o">=</span> <span class="mi">18</span>


<span class="c1"># FUNCS-------------------------------------------------------------------------</span>
<div class="viewcode-block" id="unet_width_and_layers_are_valid">
<a class="viewcode-back" href="../../../../models.html#flatiron.tf.models.unet.unet_width_and_layers_are_valid">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">unet_width_and_layers_are_valid</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
    <span class="c1"># type: (int, int) -&gt; bool</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Determines whether given UNet width and layers are valid.</span>

<span class="sd">    Args:</span>
<span class="sd">        width (int): UNet input width.</span>
<span class="sd">        layers (int): Number of UNet layers.</span>

<span class="sd">    Returns:</span>
<span class="sd">        bool: True if width and layers are compatible.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">width</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">layers</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">/=</span> <span class="mi">2</span>
        <span class="k">if</span> <span class="n">x</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="kc">True</span></div>



<div class="viewcode-block" id="conv_2d_block">
<a class="viewcode-back" href="../../../../models.html#flatiron.tf.models.unet.conv_2d_block">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">conv_2d_block</span><span class="p">(</span>
    <span class="n">input_</span><span class="p">,</span>  <span class="c1"># type: KerasTensor</span>
    <span class="n">filters</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>  <span class="c1"># type: int</span>
    <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>  <span class="c1"># type: str</span>
    <span class="n">batch_norm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># type: bool</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;he_normal&#39;</span><span class="p">,</span>  <span class="c1"># type: str</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;conv-2d-block&#39;</span><span class="p">,</span>  <span class="c1"># type: str</span>
    <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float16&#39;</span><span class="p">,</span>  <span class="c1"># type: str</span>
    <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;channels_last&#39;</span><span class="p">,</span>  <span class="c1"># type: str</span>
<span class="p">):</span>
    <span class="c1"># type: (...) -&gt; KerasTensor</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    2D Convolution block without padding.</span>

<span class="sd">    .. math::</span>
<span class="sd">        :nowrap:</span>

<span class="sd">            \begin{align}</span>
<span class="sd">                architecture &amp; \rightarrow Conv2D + ReLU + BatchNorm + Conv2D</span>
<span class="sd">                + ReLU + BatchNorm \\</span>
<span class="sd">                kernel &amp; \rightarrow (3, 3) \\</span>
<span class="sd">                strides &amp; \rightarrow (1, 1) \\</span>
<span class="sd">                padding &amp; \rightarrow same \\</span>
<span class="sd">            \end{align}</span>

<span class="sd">    .. image:: images/conv_2d_block.svg</span>
<span class="sd">      :width: 800</span>

<span class="sd">    Args:</span>
<span class="sd">        input_ (KerasTensor): Input tensor.</span>
<span class="sd">        filters (int, optional): Default: 16.</span>
<span class="sd">        activation (str, optional): Activation function. Default: relu.</span>
<span class="sd">        batch_norm (str, bool): Default: True.</span>
<span class="sd">        kernel_initializer (str, optional): Default: he_normal.</span>
<span class="sd">        name (str, optional): Layer name. Default: conv-2d-block</span>
<span class="sd">        dtype (str, optional): Model dtype. Default: float16.</span>
<span class="sd">        data_format (str, optional): Model data format. Default: channels_last.</span>

<span class="sd">    Returns:</span>
<span class="sd">        KerasTensor: Conv2D Block</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">fict</span><span class="o">.</span><span class="n">pad_layer_name</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">PAD</span><span class="p">)</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
        <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="ow">not</span> <span class="n">batch_norm</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">data_format</span><span class="o">=</span><span class="n">data_format</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">name2</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">-1&#39;</span>
    <span class="n">conv_1</span> <span class="o">=</span> <span class="n">tfl</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">-0&#39;</span><span class="p">)(</span><span class="n">input_</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">batch_norm</span><span class="p">:</span>
        <span class="n">conv_1</span> <span class="o">=</span> <span class="n">tfl</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">-1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)(</span><span class="n">conv_1</span><span class="p">)</span>
        <span class="n">name2</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">-2&#39;</span>

    <span class="n">conv_2</span> <span class="o">=</span> <span class="n">tfl</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name2</span><span class="p">)(</span><span class="n">conv_1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">batch_norm</span><span class="p">:</span>
        <span class="n">conv_2</span> <span class="o">=</span> <span class="n">tfl</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">-3&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)(</span><span class="n">conv_2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">conv_2</span></div>



<div class="viewcode-block" id="attention_gate_2d">
<a class="viewcode-back" href="../../../../models.html#flatiron.tf.models.unet.attention_gate_2d">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">attention_gate_2d</span><span class="p">(</span>
    <span class="n">query</span><span class="p">,</span>  <span class="c1"># type: KerasTensor</span>
    <span class="n">skip_connection</span><span class="p">,</span>  <span class="c1"># type: KerasTensor</span>
    <span class="n">activation_1</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>  <span class="c1"># type: str</span>
    <span class="n">activation_2</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span>  <span class="c1"># type: str</span>
    <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># type: int</span>
    <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># type: int</span>
    <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span>  <span class="c1"># type: str</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;he_normal&#39;</span><span class="p">,</span>  <span class="c1"># type: str</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;attention-gate&#39;</span><span class="p">,</span>  <span class="c1"># type: str</span>
    <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float16&#39;</span><span class="p">,</span>  <span class="c1"># type: str</span>
    <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;channels_last&#39;</span><span class="p">,</span>  <span class="c1"># type: str</span>
<span class="p">):</span>
    <span class="c1"># type: (...) -&gt; KerasTensor</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Attention gate for 2D inputs.</span>
<span class="sd">    See: https://arxiv.org/abs/1804.03999</span>

<span class="sd">    Args:</span>
<span class="sd">        query (KerasTensor): 2D Tensor of query.</span>
<span class="sd">        skip_connection (KerasTensor): 2D Tensor of features.</span>
<span class="sd">        activation_1 (str, optional): First activation. Default: &#39;relu&#39;</span>
<span class="sd">        activation_2 (str, optional): Second activation. Default: &#39;sigmoid&#39;</span>
<span class="sd">        kernel_size (int, optional): Kernel_size. Default: 1</span>
<span class="sd">        strides (int, optional): Strides. Default: 1</span>
<span class="sd">        padding (str, optional): Padding. Default: &#39;same&#39;</span>
<span class="sd">        kernel_initializer (str, optional): Kernel initializer.</span>
<span class="sd">            Default: &#39;he_normal&#39;</span>
<span class="sd">        name (str, optional): Layer name. Default: attention-gate</span>
<span class="sd">        dtype (str, optional): Model dtype. Default: float16.</span>
<span class="sd">        data_format (str, optional): Model data format. Default: channels_last.</span>

<span class="sd">    Returns:</span>
<span class="sd">        KerasTensor: 2D Attention Gate.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">fict</span><span class="o">.</span><span class="n">pad_layer_name</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">PAD</span><span class="p">)</span>
    <span class="n">filters</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">data_format</span><span class="o">=</span><span class="n">data_format</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">conv_0</span> <span class="o">=</span> <span class="n">tfl</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span>
        <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">-0&#39;</span>
    <span class="p">)(</span><span class="n">skip_connection</span><span class="p">)</span>
    <span class="n">conv_1</span> <span class="o">=</span> <span class="n">tfl</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">-1&#39;</span><span class="p">)(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">gate</span> <span class="o">=</span> <span class="n">tfl</span><span class="o">.</span><span class="n">add</span><span class="p">([</span><span class="n">conv_0</span><span class="p">,</span> <span class="n">conv_1</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">-2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">gate</span> <span class="o">=</span> <span class="n">tfl</span><span class="o">.</span><span class="n">Activation</span><span class="p">(</span><span class="n">activation_1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">-3&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)(</span><span class="n">gate</span><span class="p">)</span>
    <span class="n">gate</span> <span class="o">=</span> <span class="n">tfl</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">-4&#39;</span><span class="p">)(</span><span class="n">gate</span><span class="p">)</span>
    <span class="n">gate</span> <span class="o">=</span> <span class="n">tfl</span><span class="o">.</span><span class="n">Activation</span><span class="p">(</span><span class="n">activation_2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">-5&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)(</span><span class="n">gate</span><span class="p">)</span>
    <span class="n">gate</span> <span class="o">=</span> <span class="n">tfl</span><span class="o">.</span><span class="n">multiply</span><span class="p">([</span><span class="n">skip_connection</span><span class="p">,</span> <span class="n">gate</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">-6&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">tfl</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">gate</span><span class="p">,</span> <span class="n">query</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">-7&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>



<div class="viewcode-block" id="get_unet_model">
<a class="viewcode-back" href="../../../../models.html#flatiron.tf.models.unet.get_unet_model">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">get_unet_model</span><span class="p">(</span>
    <span class="n">input_width</span><span class="p">,</span>  <span class="c1"># type: int</span>
    <span class="n">input_height</span><span class="p">,</span>  <span class="c1"># type: int</span>
    <span class="n">input_channels</span><span class="p">,</span>  <span class="c1"># type: int</span>
    <span class="n">classes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># type: int</span>
    <span class="n">filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>  <span class="c1"># type: int</span>
    <span class="n">layers</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span>  <span class="c1"># type: int</span>
    <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;leaky_relu&#39;</span><span class="p">,</span>  <span class="c1"># type: str</span>
    <span class="n">batch_norm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># type: bool</span>
    <span class="n">output_activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span>  <span class="c1"># type: str</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;he_normal&#39;</span><span class="p">,</span>  <span class="c1"># type: str</span>
    <span class="n">attention_gates</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># type: bool</span>
    <span class="n">attention_activation_1</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>  <span class="c1"># type: str</span>
    <span class="n">attention_activation_2</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span>  <span class="c1"># type: str</span>
    <span class="n">attention_kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># type: int</span>
    <span class="n">attention_strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># type: int</span>
    <span class="n">attention_padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span>  <span class="c1"># type: str</span>
    <span class="n">attention_kernel_initializer</span><span class="o">=</span><span class="s1">&#39;he_normal&#39;</span><span class="p">,</span>  <span class="c1"># type: str</span>
    <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float16&#39;</span><span class="p">,</span>  <span class="c1"># type: str</span>
    <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;channels_last&#39;</span><span class="p">,</span>  <span class="c1"># type: str</span>
<span class="p">):</span>
    <span class="c1"># type: (...) -&gt; tfmodels.Model</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    UNet model for 2D semantic segmentation.</span>

<span class="sd">    see: https://arxiv.org/abs/1505.04597</span>
<span class="sd">    see: https://arxiv.org/pdf/1411.4280.pdf</span>
<span class="sd">    see: https://arxiv.org/abs/1804.03999</span>

<span class="sd">    Args:</span>
<span class="sd">        input_width (int): Input width.</span>
<span class="sd">        input_height (int): Input height.</span>
<span class="sd">        input_channels (int): Input channels.</span>
<span class="sd">        classes (int, optional): Number of output classes. Default: 1.</span>
<span class="sd">        filters (int, optional): Number of filters for initial con 2d block.</span>
<span class="sd">            Default: 32.</span>
<span class="sd">        layers (int, optional): Total number of layers. Default: 9.</span>
<span class="sd">        activation (KerasTensor, optional): Activation function to be used.</span>
<span class="sd">            Default: leaky_relu.</span>
<span class="sd">        batch_norm (KerasTensor, optional): Use batch normalization.</span>
<span class="sd">            Default: True.</span>
<span class="sd">        output_activation (KerasTensor, optional): Output activation function.</span>
<span class="sd">            Default: sigmoid.</span>
<span class="sd">        kernel_initializer (KerasTensor, optional): Default: he_normal.</span>
<span class="sd">        attention_gates (KerasTensor, optional): Use attention gates.</span>
<span class="sd">            Default: False.</span>
<span class="sd">        attention_activation_1 (str, optional): First activation.</span>
<span class="sd">            Default: &#39;relu&#39;</span>
<span class="sd">        attention_activation_2 (str, optional): Second activation.</span>
<span class="sd">            Default: &#39;sigmoid&#39;</span>
<span class="sd">        attention_kernel_size (int, optional): Kernel_size. Default: 1</span>
<span class="sd">        attention_strides (int, optional): Strides. Default: 1</span>
<span class="sd">        attention_padding (str, optional): Padding. Default: &#39;same&#39;</span>
<span class="sd">        attention_kernel_initializer (str, optional): Kernel initializer.</span>
<span class="sd">            Default: &#39;he_normal&#39;</span>
<span class="sd">        dtype (str, optional): Model dtype. Default: float16.</span>
<span class="sd">        data_format (str, optional): Model data format. Default: channels_last.</span>

<span class="sd">    Raises:</span>
<span class="sd">        EnforceError: If input_width is not even.</span>
<span class="sd">        EnforceError: If input_height is not even.</span>
<span class="sd">        EnforceError: If layers is not an odd integer greater than 2.</span>
<span class="sd">        EnforceError: If input_width and layers are not compatible.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tfmodels.Model: UNet model.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># shape</span>
    <span class="n">msg</span> <span class="o">=</span> <span class="s1">&#39;Input width and height must be equal, even numbers. &#39;</span>
    <span class="n">msg</span> <span class="o">+=</span> <span class="sa">f</span><span class="s1">&#39;Given shape: (</span><span class="si">{</span><span class="n">input_width</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">input_height</span><span class="si">}</span><span class="s1">).&#39;</span>
    <span class="n">Enforce</span><span class="p">(</span><span class="n">input_width</span> <span class="o">%</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;==&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>
    <span class="n">Enforce</span><span class="p">(</span><span class="n">input_height</span> <span class="o">%</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;==&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>
    <span class="n">Enforce</span><span class="p">(</span><span class="n">input_width</span><span class="p">,</span> <span class="s1">&#39;==&#39;</span><span class="p">,</span> <span class="n">input_height</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

    <span class="c1"># layers</span>
    <span class="n">msg</span> <span class="o">=</span> <span class="s1">&#39;Layers must be an odd integer greater than 2. &#39;</span>
    <span class="n">msg</span> <span class="o">+=</span> <span class="sa">f</span><span class="s1">&#39;Given value: </span><span class="si">{</span><span class="n">layers</span><span class="si">}</span><span class="s1">.&#39;</span>
    <span class="n">Enforce</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="s1">&#39;instance of&#39;</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>
    <span class="n">Enforce</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="s1">&#39;&gt;=&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>
    <span class="n">Enforce</span><span class="p">(</span><span class="n">layers</span> <span class="o">%</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;==&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

    <span class="c1"># valid width and layers</span>
    <span class="n">msg</span> <span class="o">=</span> <span class="s1">&#39;Given input_width and layers are not compatible. &#39;</span>
    <span class="n">msg</span> <span class="o">+=</span> <span class="sa">f</span><span class="s1">&#39;Input_width: </span><span class="si">{</span><span class="n">input_width</span><span class="si">}</span><span class="s1">. Layers: </span><span class="si">{</span><span class="n">layers</span><span class="si">}</span><span class="s1">.&#39;</span>
    <span class="n">Enforce</span><span class="p">(</span>
        <span class="n">unet_width_and_layers_are_valid</span><span class="p">(</span><span class="n">input_width</span><span class="p">,</span> <span class="n">layers</span><span class="p">),</span> <span class="s1">&#39;==&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="n">msg</span>
    <span class="p">)</span>
    <span class="c1"># --------------------------------------------------------------------------</span>

    <span class="n">n</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">encode_layers</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># input layer</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_width</span><span class="p">,</span> <span class="n">input_height</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">)</span>
    <span class="n">input_</span> <span class="o">=</span> <span class="n">tfl</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

    <span class="c1"># encode layers</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">input_</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="c1"># conv backend of layer</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">conv_2d_block</span><span class="p">(</span>
            <span class="n">input_</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
            <span class="n">batch_norm</span><span class="o">=</span><span class="n">batch_norm</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
            <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;encode-block_</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s1">02d</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">data_format</span><span class="o">=</span><span class="n">data_format</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">encode_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># downsample</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">fict</span><span class="o">.</span><span class="n">pad_layer_name</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;downsample_</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s1">02d</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">PAD</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tfl</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">(</span>
            <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="n">data_format</span><span class="p">,</span>
        <span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">filters</span> <span class="o">*=</span> <span class="mi">2</span>

    <span class="c1"># middle layer</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">fict</span><span class="o">.</span><span class="n">pad_layer_name</span><span class="p">(</span><span class="s1">&#39;middle-block_00&#39;</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">PAD</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">conv_2d_block</span><span class="p">(</span>
        <span class="n">input_</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
        <span class="n">batch_norm</span><span class="o">=</span><span class="n">batch_norm</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># decode layers</span>
    <span class="n">decode_layers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">encode_layers</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">decode_layers</span><span class="p">[:</span><span class="n">n</span><span class="p">]):</span>
        <span class="n">filters</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">filters</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># upsample</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">fict</span><span class="o">.</span><span class="n">pad_layer_name</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;upsample_</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s1">02d</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">PAD</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tfl</span><span class="o">.</span><span class="n">Conv2DTranspose</span><span class="p">(</span>
            <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
            <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
            <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">data_format</span><span class="o">=</span><span class="n">data_format</span><span class="p">,</span>
        <span class="p">)(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># attention gate</span>
        <span class="k">if</span> <span class="n">attention_gates</span><span class="p">:</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">fict</span><span class="o">.</span><span class="n">pad_layer_name</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;attention-gate_</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s1">02d</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">PAD</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">attention_gate_2d</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span>
                <span class="n">layer</span><span class="p">,</span>
                <span class="n">activation_1</span><span class="o">=</span><span class="n">attention_activation_1</span><span class="p">,</span>
                <span class="n">activation_2</span><span class="o">=</span><span class="n">attention_activation_2</span><span class="p">,</span>
                <span class="n">kernel_size</span><span class="o">=</span><span class="n">attention_kernel_size</span><span class="p">,</span>
                <span class="n">strides</span><span class="o">=</span><span class="n">attention_strides</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="n">attention_padding</span><span class="p">,</span>
                <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">attention_kernel_initializer</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">data_format</span><span class="o">=</span><span class="n">data_format</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">fict</span><span class="o">.</span><span class="n">pad_layer_name</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;concat_</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s1">02d</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">PAD</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">tfl</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">layer</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># conv backend of layer</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">conv_2d_block</span><span class="p">(</span>
            <span class="n">input_</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
            <span class="n">batch_norm</span><span class="o">=</span><span class="n">batch_norm</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
            <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;decode-block_</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s1">02d</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">data_format</span><span class="o">=</span><span class="n">data_format</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">tfl</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span>
        <span class="n">classes</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="n">output_activation</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;output&#39;</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="n">data_format</span><span class="p">,</span>
    <span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">tfmodels</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">input_</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">output</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span></div>



<span class="c1"># CONFIG------------------------------------------------------------------------</span>
<div class="viewcode-block" id="UNetConfig">
<a class="viewcode-back" href="../../../../models.html#flatiron.tf.models.unet.UNetConfig">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">UNetConfig</span><span class="p">(</span><span class="n">pyd</span><span class="o">.</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Configuration for UNet model.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        input_width (int): Input width.</span>
<span class="sd">        input_height (int): Input height.</span>
<span class="sd">        input_channels (int): Input channels.</span>
<span class="sd">        classes (int, optional): Number of output classes. Default: 1.</span>
<span class="sd">        filters (int, optional): Number of filters for initial con 2d block.</span>
<span class="sd">            Default: 16.</span>
<span class="sd">        layers (int, optional): Total number of layers. Default: 9.</span>
<span class="sd">        activation (KerasTensor, optional): Activation function to be used.</span>
<span class="sd">            Default: relu.</span>
<span class="sd">        batch_norm (KerasTensor, optional): Use batch normalization.</span>
<span class="sd">            Default: True.</span>
<span class="sd">        output_activation (KerasTensor, optional): Output activation function.</span>
<span class="sd">            Default: sigmoid.</span>
<span class="sd">        kernel_initializer (KerasTensor, optional): Default: he_normal.</span>
<span class="sd">        attention_gates (KerasTensor, optional): Use attention gates.</span>
<span class="sd">            Default: False.</span>
<span class="sd">        attention_activation_1 (str, optional): First activation.</span>
<span class="sd">            Default: &#39;relu&#39;</span>
<span class="sd">        attention_activation_2 (str, optional): Second activation.</span>
<span class="sd">            Default: &#39;sigmoid&#39;</span>
<span class="sd">        attention_kernel_size (int, optional): Kernel_size. Default: 1</span>
<span class="sd">        attention_strides (int, optional): Strides. Default: 1</span>
<span class="sd">        attention_padding (str, optional): Padding. Default: &#39;same&#39;</span>
<span class="sd">        attention_kernel_initializer (str, optional): Kernel initializer.</span>
<span class="sd">            Default: &#39;he_normal&#39;</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">input_width</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">pyd</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">ge</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span>
    <span class="n">input_height</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">pyd</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">ge</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span>
    <span class="n">input_channels</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">pyd</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">ge</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span>
    <span class="n">classes</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">pyd</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">ge</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">filters</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">pyd</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">ge</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">16</span>
    <span class="n">layers</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">pyd</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">ge</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span> <span class="n">pyd</span><span class="o">.</span><span class="n">AfterValidator</span><span class="p">(</span><span class="n">vd</span><span class="o">.</span><span class="n">is_odd</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">9</span>
    <span class="n">activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span>
    <span class="n">batch_norm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">output_activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;sigmoid&#39;</span>
    <span class="n">kernel_initializer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;he_normal&#39;</span>
    <span class="n">attention_gates</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">attention_activation_1</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span>
    <span class="n">attention_activation_2</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;sigmoid&#39;</span>
    <span class="n">attention_kernel_size</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">pyd</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">ge</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">attention_strides</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">pyd</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">ge</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">attention_padding</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">pyd</span><span class="o">.</span><span class="n">AfterValidator</span><span class="p">(</span><span class="n">vd</span><span class="o">.</span><span class="n">is_padding</span><span class="p">)]</span> <span class="o">=</span> <span class="s1">&#39;same&#39;</span>
    <span class="n">attention_kernel_initializer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;he_normal&#39;</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;float16&#39;</span>
    <span class="n">data_format</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;channels_last&#39;</span></div>



<span class="c1"># PIPELINE----------------------------------------------------------------------</span>
<div class="viewcode-block" id="UNetPipeline">
<a class="viewcode-back" href="../../../../models.html#flatiron.tf.models.unet.UNetPipeline">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">UNetPipeline</span><span class="p">(</span><span class="n">ficp</span><span class="o">.</span><span class="n">PipelineBase</span><span class="p">):</span>
<div class="viewcode-block" id="UNetPipeline.model_config">
<a class="viewcode-back" href="../../../../models.html#flatiron.tf.models.unet.UNetPipeline.model_config">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">model_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># type: () -&gt; type[UNetConfig]</span>
        <span class="k">return</span> <span class="n">UNetConfig</span></div>


<div class="viewcode-block" id="UNetPipeline.model_func">
<a class="viewcode-back" href="../../../../models.html#flatiron.tf.models.unet.UNetPipeline.model_func">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">model_func</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># type: () -&gt; tfmodels.Model</span>
        <span class="k">return</span> <span class="n">get_unet_model</span></div>
</div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Alex Braun &lt;alexander.g.braun@gmail.com&gt;.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>